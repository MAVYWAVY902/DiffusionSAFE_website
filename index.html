<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Diffusion-SAFE: Shared Autonomy Framework with
        Diffusion for Safe Human-to-Robot Driving Handover.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Diffusion-SAFE: Shared Autonomy Framework with
    Diffusion for Safe Human-to-Robot Driving Handover</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/stanford.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://arm.stanford.edu">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">Diffusion-SAFE: Shared Autonomy Framework with
            Diffusion for Safe Human-to-Robot Driving Handover</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://arm.stanford.edu/people">Yunxin Fan</a>,</span>
            <span class="author-block">
              <a href="https://monroekennedy3.com">Monroe Kennedy III</a></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Stanford University</span>
            <!-- <span class="author-block"><sup>2</sup>Google Research</span> -->
          </div>

          <!-- <div class="column has-text-centered"> -->
            <!-- <div class="publication-links"> -->
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->
              <!-- <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span> -->
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            <!-- </div> -->

          <!-- </div> -->
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <div class="rows is-full-width">

      <!-- Visual Effects. -->
      <div class="rows">
        <div class="content">
          <!-- <h2 class="title is-3">Simulation Demos</h2> -->

          <div class="columns is-multiline">
            <div class="column is-half has-text-centered">
              <img id="teaser-image" src="./static/images/splash_screenshot.png" alt="splash figure" width="100%">
            </div>
            <div class="column is-half has-text-centered">
              <video id="scene2" autoplay muted loop playsinline height="100%">
                <source src="./static/videos/diffuse.mp4"
                        type="video/mp4">
              </video>
            </div>
            <p>
              Closed-loop framework with two diffusion model-based policies: an
              evaluator to predict human intent, and a copilot to provide optimal trajectories and ensure smooth control transitions during safety-critical situations.
            </p>
        </div>
      </div>
    </div>


    <section class="section">
      <div class="container is-max-desktop">
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-full-width">
            <h2 class="title is-4">Abstract</h2>
            <div class="content has-text-justified">
              <p>
                Safe handover in shared autonomy for vehicle control is well-established in modern vehicles. However, avoiding
                accidents often requires action several seconds in advance. This
                necessitates understanding human driver behavior and an expert
                control strategy for seamless intervention when a collision or
                unsafe state is predicted. 
              </p>
              <p>
                We propose <b>Diffusion-SAFE</b>, a closed-loop shared autonomy framework leveraging diffusion models
                to: (1) predict human driving behavior for detection of potential risks, (2) generate safe expert trajectories, and (3) enable smooth
                handovers by blending human and expert policies over a short
                time horizon. Unlike prior works which use engineered score
                functions to rate driving performance, our approach enables both
                performance evaluation and optimal action sequence generation
                from demonstrations. By adjusting the forward and reverse
                processes of the diffusion-based copilot, our method ensures
                a gradual transition of control authority, by mimicking the
                drivers behavior before intervention, which mitigates abrupt
                takeovers, leading to smooth transitions.
              </p>
              <p>
                We evaluated <b>Diffusion-SAFE</b> in both simulation (CarRacing-v0) and real-world (ROS-
                based race car), measuring human driving similarity, safety, and
                computational efficiency. Results demonstrate a <b>98.5%</b> successful
                handover rate, highlighting the framework's effectiveness in progressively correcting human actions and continuously sampling
                optimal robot actions. Videos and code will be released upon
                publication.
              </p>
            </div>
          </div>
        </div>
        <!--/ Abstract. -->
    
        <!-- Paper video. -->
        <!-- <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Video</h2>
            <div class="publication-video">
              <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                      frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
            </div>
          </div>
        </div> -->
        <!--/ Paper video. -->
      </div>
    </section>


<section class="framework">
  <div class="container is-max-desktop">
    <div class="hero-body has-text-centered">
      <img id="teaser-image" src="./static/images/framework.png" alt="Framework Image" width="100%">
      <p>
        <b>Diffusion-SAFE</b> architecture: The evaluator model processes observations and action sequences, sampling future action
        sequences aligned with human intent in a simulated environment. The copilot model generates and executes expert action sequences when the human
        performance score falls below a predefined threshold \( \tau_{NLL} \).
      </p>
      <!-- MathJax script for rendering LaTeX -->
      <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
      </script>
    </div>
  </div>
</section>







<section class="section">
  <div class="container is-max-desktop">

    <!-- 📌 Model Architecture -->
    <div class="column is-full-width">
      <h2 class="title is-4">Model Architecture</h2>
      <div class="content has-text-justified">
        <p>
          <b>Noise Estimator Architecture:</b> U-Net design with residual connections, 
          positional embedding of step \( t \), and conditioning vector 
          \( \mathbf{C}_{0:t_{\text{obs}}} \). Double convolution block (DC in the figure).
        </p>

        <!-- MathJax script for rendering LaTeX -->
        <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
        </script>
      </div>
      <div class="has-text-centered">
        <video id="unet-image" autoplay muted loop playsinline width="100%">
          <source src="./static/videos/unet_video.mp4" type="video/mp4">
        </video>
      </div>
    </div>
    <br/>


    <!-- 📌 Simulation Four Scenes -->
    <div class="column is-full-width">
      <h2 class="title is-4">Simulation Demos</h2>
      <div class="content has-text-justified"></div>
      <p>
        Here we showcase four different simulated scenes randomly generated in Gym CarRacing-v0.
      </p>

      <br> <!-- more space -->
      <div style="margin-bottom: 10px;"></div> 

      <div class="columns is-multiline">
        <div class="column is-half has-text-centered">
          <video id="scene1" autoplay controls muted loop playsinline width="100%">
            <source src="./static/videos/scene1.mp4" type="video/mp4">
          </video>
        </div>
        <div class="column is-half has-text-centered">
          <video id="scene2" autoplay controls muted loop playsinline width="100%">
            <source src="./static/videos/scene2.mp4" type="video/mp4">
          </video>
        </div>
        <div class="column is-half has-text-centered">
          <video id="scene3" autoplay controls muted loop playsinline width="100%">
            <source src="./static/videos/scene3.mp4" type="video/mp4">
          </video>
        </div>
        <div class="column is-half has-text-centered">
          <video id="scene4" autoplay controls muted loop playsinline width="100%">
            <source src="./static/videos/scene4.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
    <br/>

    <!-- 📌 Simulation Demos (Handover) -->
    <div class="column is-full-width">
      <h2 class="title is-4">Partial Diffusion as Handover Process</h2>
      <p>
        By changing \( \gamma \), we can adjust the balance between preserving human input and following the safe behavior of the copilot: 
        when \( \gamma \) is small, human intent is well preserved, and thus with limited alignment to \( P_{\text{copilot}} \); 
        in contrast, larger values \( \gamma \) would lead the system to prioritize aligning with the copilot policy over human input.
      </p>
      <br> <!-- more space -->
      <div style="margin-bottom: 1px;"></div> 
      <p>
        Comparison of Our Partial Diffusion Method and Simple Blending in the Handover Process:
      </p>
      <!-- MathJax script for rendering LaTeX -->
      <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
      </script>

      <br> <!-- more space -->
      <div style="margin-bottom: 10px;"></div> 


      <div class="columns is-multiline">
        <div class="column is-half has-text-centered">
          <video id="handover1" autoplay controls muted loop playsinline width="100%">
            <source src="./static/videos/our_handover.mp4" type="video/mp4">
          </video>
          <p>
            <b>Our approach (via forward diffusion ratio</b> \( \gamma \) <b>)</b>
        </p>
        
        <!-- MathJax script for rendering LaTeX -->
        <script type="text/javascript" async
          src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
        </script>
        

         
        </div>
        <div class="column is-half has-text-centered">
          <video id="handover2" autoplay controls muted loop playsinline width="100%">
            <source src="./static/videos/simple_blend.mp4" type="video/mp4">
          </video>
          <p>
            <b>Simply blend:</b> 
            &nbsp;
            \( \mathbf{a}_{blend} = k \mathbf{a}_{H} + (1 - k) \mathbf{a}_{copilot} \)
          </p>
        
          <!-- MathJax script for rendering LaTeX -->
          <script type="text/javascript" async
            src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
          </script>
        

        </div>
      </div>
    </div>
    <br/>


    <!-- 📌 Ablation -->
    <div class="column is-full-width">
      <h2 class="title is-4">Ablations</h2>
      
      <p>
        Ablation studies are conducted for both the evaluator and the copilot. 
        For the evaluator, the observation horizon = 20, and the prediction horizon = 30, unless stated otherwise. 
        For the copilot, the observation horizon = 10, and the prediction horizon = 15, unless stated otherwise. 
        The horizon unit is measured in steps, where each step corresponds to 0.1s. \( \textbf{Bold} \) indicates the best result, while \( \textit{Italic} \) indicates the second-best result.
      </p>
    
    
      <!-- MathJax script for rendering LaTeX -->
      <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
      </script>
    
      <br> <!-- add space -->
      <div style="margin-bottom: 20px;"></div> 

      <!-- 📌 two videos in one rows -->
      <div class="columns is-multiline">
        <div class="column is-half has-text-centered">
          <img id="mocap-image" src="./static/images/evaluator_ablation.png" alt="evaluator_ablation" width="100%">
          <p><b>Ablation table for the evaluator model</b></p>
        
        <!-- MathJax script for rendering LaTeX -->
        <script type="text/javascript" async
          src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
        </script>
        </p>
        </div>
        <div class="column is-half has-text-centered">
          <img id="mocap-image" src="./static/images/copilot_ablation.png" alt="evaluator_ablation" width="100%">
        <p><b>Ablation table for the copilot model</b></p>
        </div>
      </div>
    </div>


      <!-- 📌 Baseline -->
      <br> <!-- add space -->
      <div style="margin-bottom: 10px;"></div> 
      <div class="column is-full-width">
        <h2 class="title is-4">Comparison with Baselines</h2>
        
        <p>
          In this work, we utilize the ability of the diffusion policy to inherently express multimodal distributions. 
          Our method is compared to the following multimodal methods: <a
          href="https://robomimic.github.io">LSTM-GMM</a> and <a
          href="https://mahis.life/bet">Behavior Transformers models (BET)</a>. 
          The results are summarized in the following tables.
          The horizon unit is measured in steps, where each step corresponds to 0.1s. \( \textbf{Bold} \) indicates the best result, while \( \textit{Italic} \) indicates the second-best result.
        </p>

        <!-- MathJax script for rendering LaTeX -->
        <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
        </script>
        
        <br> <!-- add space -->
        <div style="margin-bottom: 10px;"></div> 

  
        <!-- 📌 two figures in one row -->
        <div class="columns is-multiline">
          <div class="column is-half has-text-centered">
            <img id="mocap-image" src="./static/images/evaluator_comparison.png" alt="evaluator_comparison" width="100%">
            <p><b>Baseline Comparison table for the evaluator model</b></p>
          </div>
          <div class="column is-half has-text-centered">
            <img id="mocap-image" src="./static/images/copilot_comparison.png" alt="copilot_comparison" width="100%">
            <p><b>Baseline Comparison table for the copilot model</b></p>
          </div>
        </div>
      </div>

    <!-- 📌 Motion Capture Visualization -->
    <br> <!-- add space -->
    <div style="margin-bottom: 10px;"></div> 
    <div class="column is-full-width">
      <h2 class="title is-4">Real-world Experiment Setup and Dataset Collection Pipeline</h2>
      
      <p>
        We conducted real-world experiments on a ROS-based race car equipped with a Jetson Orin Nano for onboard processing. A Windows11 computer running Motive software streamed data from a 13-camera OptiTrack motion capture system.
      </p>

      <br> <!-- more space -->
      <div style="margin-bottom: 2px;"></div> 

      <p>
        To be detailed, we use Mocap system to obtain real-time poses in the world frame of the car,
        convert poses in the world frame into the pixel frame, and use pose data (in the pixel frame) to real-time crop image patches for dataset collection.

      </p>

      <br> <!-- more space -->
      <div style="margin-bottom: 10px;"></div> 

      <div class="has-text-centered">
        <img id="mocap-image" src="./static/images/mocap.png" alt="Motion Capture Visualization" width="80%">
      </div>
    </div>


    <!-- 📌 Real-world Demo -->
    <div class="column is-full-width">
      <h2 class="title is-4">Real-world Demo</h2>
      <p>
        Real-world demo on an unseen map:
      </p>

      <br> <!-- more space -->
      <div style="margin-bottom: 10px;"></div> 

      <div class="has-text-centered">
        <video id="teaser" autoplay muted loop playsinline width="100%">
          <source src="./static/videos/teaser.mp4" type="video/mp4">
        </video>
      </div>
    </div>
    <br/>

  </div>
</section>



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{DiffusionSAFE,
  author    = {Yunxin Fan and Monroe Kennedy III},
  title     = {Diffusion-SAFE: Shared Autonomy Framework with Diffusion for Safe Human-to-Robot Driving Handover},
  journal   = {arXiv},
  year      = {2025},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <!-- <a class="icon-link"
   href="./static/videos/nerfies_paper.pdf">
  <i class="fas fa-file-pdf"></i>
</a> -->
      <a class="icon-link" href="https://github.com/armlabstanford"
        class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            We appreciate <a
              href="https://nerfies.github.io">Nerfies</a> for providing
            the template for this website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
